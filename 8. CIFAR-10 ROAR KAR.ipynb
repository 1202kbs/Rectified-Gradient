{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, math, collections\n",
    "\n",
    "if not os.path.exists('CIFAR10_data'):\n",
    "    \n",
    "    !wget https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
    "    !mkdir CIFAR10_data\n",
    "    !tar -xf cifar-10-python.tar.gz -C CIFAR10_data\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from utils import save, scale, unpickle, batch_run, preprocess\n",
    "from deepexplain.tensorflow import DeepExplain\n",
    "from models import CIFAR_CNN\n",
    "from trainer import Trainer\n",
    "\n",
    "datadir = 'CIFAR10_data/cifar-10-batches-py/'\n",
    "batches_train = sorted([datadir + batch for batch in os.listdir(datadir) if 'data_batch' in batch], key=lambda x: int(x[-1]))\n",
    "batch_test = datadir + 'test_batch'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(5)):\n",
    "    \n",
    "    batch = unpickle(batches_train[i])\n",
    "\n",
    "    if i == 0:\n",
    "        data = batch[b'data'].astype(np.float32)\n",
    "        cifar = np.transpose(np.reshape(data, [-1,3,32,32]), [0,2,3,1])\n",
    "        labels = batch[b'labels']\n",
    "    else:\n",
    "        data = batch[b'data'].astype(np.float32)\n",
    "        cifar = np.concatenate((cifar, np.transpose(np.reshape(data, [-1,3,32,32]), [0,2,3,1])), axis=0)\n",
    "        labels = np.concatenate((labels, batch[b'labels']), axis=0)\n",
    "\n",
    "test_batch = unpickle(batch_test)\n",
    "cifar_test = np.transpose(np.reshape(test_batch[b'data'], [-1,3,32,32]), [0,2,3,1])\n",
    "labels_test = np.array(test_batch[b'labels'])\n",
    "\n",
    "data_train = (cifar / 127.5 - 1.0, labels)\n",
    "data_test = (cifar_test / 127.5 - 1.0, labels_test)\n",
    "\n",
    "cifar_mean = np.mean(cifar, axis=(0,1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_remove(images, percentile, keep=False):\n",
    "    \n",
    "    images = np.copy(images)\n",
    "    \n",
    "    mask = np.random.binomial(1, (100 - percentile) / 100, size=images.shape[:-1])\n",
    "    \n",
    "    if keep:\n",
    "        images[mask == 1] = cifar_mean\n",
    "    else:\n",
    "        images[mask == 0] = cifar_mean\n",
    "    \n",
    "    return images\n",
    "\n",
    "\n",
    "def remove(images, attributions, percentile, keep=False, random=False):\n",
    "    '''\n",
    "    images       : tensor of shape [N,H,W,C]\n",
    "    attributions : tensor of shape [N,H,W]\n",
    "    percentile   : scalar between 0 and 100, inclusive\n",
    "    keep         : if true keep q percent; otherwise remove q percent\n",
    "    '''\n",
    "    \n",
    "    images = np.copy(images)\n",
    "    \n",
    "    thresholds = np.percentile(attributions, 100 - percentile, axis=(1,2), keepdims=True)\n",
    "    \n",
    "    if keep:\n",
    "        images[attributions < thresholds] = cifar_mean\n",
    "    else:\n",
    "        images[attributions > thresholds] = cifar_mean\n",
    "    \n",
    "    return images\n",
    "\n",
    "\n",
    "def occlude_dataset(DNN, de, attribution, percentiles, test=False, keep=False, random=False, batch_size=1000, savedir=''):\n",
    "    \n",
    "    if test:\n",
    "        Xs = cifar_test\n",
    "        ys = labels_test\n",
    "    else:\n",
    "        Xs = cifar\n",
    "        ys = labels\n",
    "    \n",
    "    total_batch = math.ceil(len(Xs) / batch_size)\n",
    "    \n",
    "    if not random:\n",
    "    \n",
    "        hmaps = []\n",
    "\n",
    "        for i in tqdm(range(total_batch)):\n",
    "\n",
    "            batch_xs = Xs[i*batch_size:(i+1)*batch_size]\n",
    "            batch_xs_scaled = scale(batch_xs)\n",
    "\n",
    "            if 'edge' in attribution:\n",
    "                attrs = Canny(batch_xs)\n",
    "            else:\n",
    "                \n",
    "                if 'rectgrad' in attribution:\n",
    "                    attrs = de.explain(attribution, DNN.yv, DNN.X, batch_xs_scaled)\n",
    "                    attrs = np.sum(np.where(attrs > 0, attrs, 0.0), axis=-1)\n",
    "                else:\n",
    "                    attrs = preprocess(de.explain(attribution, DNN.yv, DNN.X, batch_xs_scaled), 0, 100)\n",
    "            \n",
    "            # Add small noise so np.percentile functions correctly in feature removal process\n",
    "            attrs += np.random.normal(scale=1e-4, size=attrs.shape)\n",
    "            hmaps.append(attrs)\n",
    "\n",
    "        hmaps = np.concatenate(hmaps, axis=0)\n",
    "    \n",
    "    for percentile in tqdm(percentiles):\n",
    "        \n",
    "        dataset = []\n",
    "        \n",
    "        for i in range(total_batch):\n",
    "            \n",
    "            batch_xs, batch_ys = Xs[i*batch_size:(i+1)*batch_size], ys[i*batch_size:(i+1)*batch_size]\n",
    "            \n",
    "            if random:\n",
    "                occluded_images = random_remove(batch_xs, percentile, keep)\n",
    "            else:\n",
    "                batch_attrs = hmaps[i*batch_size:(i+1)*batch_size]\n",
    "                occluded_images = remove(batch_xs, batch_attrs, percentile, keep)\n",
    "            \n",
    "            dataset.append(scale(occluded_images))\n",
    "        \n",
    "        save(np.concatenate(dataset, axis=0), savedir + '{}_{}_{}.pickle'.format('test' if test else 'train', attribution, percentile))\n",
    "\n",
    "\n",
    "def roar_kar(keep, train_only=False):\n",
    "    \n",
    "    logdir = 'tf_logs/standard/'\n",
    "    \n",
    "    def get_savedir():\n",
    "        \n",
    "        savedir = logdir.replace('tf_logs', 'KAR' if keep else 'ROAR')\n",
    "        \n",
    "        if not os.path.exists(savedir):\n",
    "            \n",
    "            os.makedirs(savedir)\n",
    "        \n",
    "        return savedir\n",
    "    \n",
    "    percentiles = [10, 30, 50, 70, 90]\n",
    "    \n",
    "    heavy_methods = ['SmoothGrad', 'IntegGrad', 'DeepLIFT']\n",
    "\n",
    "    attribution_methods = [\n",
    "                           ('Random'           , 'zero'),\n",
    "                           ('RectGrad'         , 'rectgrad'),\n",
    "                           ('Saliency Map'     , 'saliency'),\n",
    "                           ('Guided BP'        , 'guidedbp'),\n",
    "                           ('SmoothGrad'       , 'smoothgrad'),\n",
    "                           ('Gradient * Input' , 'grad*input'),\n",
    "                           ('IntegGrad'        , 'intgrad'),\n",
    "                           ('DeepLIFT'         , 'deeplift')\n",
    "                          ]\n",
    "\n",
    "    attribution_methods = collections.OrderedDict(attribution_methods)\n",
    "    \n",
    "    if not train_only:\n",
    "    \n",
    "        tf.reset_default_graph()\n",
    "\n",
    "        sess = tf.InteractiveSession()\n",
    "\n",
    "        with DeepExplain(session=sess, graph=sess.graph) as de:\n",
    "\n",
    "            DNN = CIFAR_CNN(logdir, 'CNN')\n",
    "            DNN.load(sess)\n",
    "\n",
    "            for k, v in attribution_methods.items():\n",
    "\n",
    "                if k in heavy_methods:\n",
    "                    batch_size = 2500\n",
    "                else:\n",
    "                    batch_size = 5000\n",
    "\n",
    "                occlude_dataset(DNN, de, v, percentiles, False, keep, k == 'Random', batch_size, get_savedir())\n",
    "                occlude_dataset(DNN, de, v, percentiles, True,  keep, k == 'Random', batch_size, get_savedir())\n",
    "\n",
    "        sess.close()\n",
    "    \n",
    "    ress = {k : [] for k in attribution_methods.keys()}\n",
    "    \n",
    "    for _ in range(3):\n",
    "    \n",
    "        for k, v in attribution_methods.items():\n",
    "            \n",
    "            res = []\n",
    "\n",
    "            for p in percentiles:\n",
    "                \n",
    "                occdir = get_savedir() + '{}_{}_{}.pickle'.format('{}', v, p)\n",
    "                data_train = (unpickle(occdir.format('train')), labels)\n",
    "                data_test = (unpickle(occdir.format('test')), labels_test)\n",
    "                \n",
    "                tf.reset_default_graph()\n",
    "\n",
    "                DNN = CIFAR_CNN('tf_logs/roar_kar/', 'CNN')\n",
    "\n",
    "                sess = tf.InteractiveSession()\n",
    "                sess.run(tf.global_variables_initializer())\n",
    "\n",
    "                trainer = Trainer(sess, DNN, data_train)\n",
    "                trainer.train(20, p_epochs=30)\n",
    "\n",
    "                acc = DNN.evaluate(sess, data_test)\n",
    "\n",
    "                print('{}{} | Test Accuracy : {:.5f}'.format(k, p, acc))\n",
    "\n",
    "                res.append(acc)\n",
    "\n",
    "                sess.close()\n",
    "            \n",
    "            ress[k].append(res)\n",
    "    \n",
    "    res_mean = {k: np.mean(v, axis=0) for k, v in ress.items()}\n",
    "    \n",
    "    print(res_mean)\n",
    "    \n",
    "    return res_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = roar_kar(keep=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
